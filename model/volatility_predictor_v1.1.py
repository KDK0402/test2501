import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import matplotlib.pyplot as plt
from matplotlib import font_manager, rc
import matplotlib.dates as mdates
from tabulate import tabulate
from sklearn.metrics import mean_absolute_error, r2_score
import streamlit as st
import plotly.graph_objects as go

# í•œê¸€ í°íŠ¸ ì„¤ì •
font_path = "C:/Windows/Fonts/malgun.ttf"
font = font_manager.FontProperties(fname=font_path).get_name()
rc('font', family=font)
plt.rcParams['axes.unicode_minus'] = False

# ë°ì´í„° ë¡œë“œ
df = pd.read_excel('C:/Users/infomax/Documents/10yKTB.xlsx', skiprows=3, usecols=[0,1])
df.columns = ['Date', 'Rate']

# ë‚ ì§œ í˜•ì‹ ë³€í™˜ ë° ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œ ì œê±°
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')  # ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œëŠ” NaTë¡œ ë³€í™˜
df = df.dropna(subset=['Date'])  # NaT ê°’ì´ ìˆëŠ” í–‰ ì œê±°

# ë°ì´í„°ë¥¼ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (ê³¼ê±° -> í˜„ì¬)
df = df.sort_values('Date', ascending=True).reset_index(drop=True)

# ë°ì´í„° í™•ì¸ ì¶œë ¥
print("\n=== ë°ì´í„° í™•ì¸ ===")
print(f"ë°ì´í„° ì‹œì‘ì¼: {df['Date'].min().strftime('%Y-%m-%d')}")
print(f"ë°ì´í„° ì¢…ë£Œì¼: {df['Date'].max().strftime('%Y-%m-%d')}")
print(f"ì´ ë°ì´í„° ìˆ˜: {len(df)}")

# ì¼ê°„ ë³€í™”ìœ¨ ê³„ì‚° (basis point ë‹¨ìœ„ë¡œ ë³€í™˜: 0.01 = 1bp)
df['Daily_Change'] = df['Rate'].diff() * 100  # bp ë‹¨ìœ„ë¡œ ë³€í™˜

# ë³€ë™ì„± ê³„ì‚° (20ì¼ ë¡¤ë§ ìœˆë„ìš°, bp ë‹¨ìœ„)
df['Volatility'] = df['Daily_Change'].rolling(window=20).std()  # ë‹¨ìˆœ í‘œì¤€í¸ì°¨ë§Œ ì‚¬ìš©
df = df.dropna()

print("\n=== ë³€ë™ì„± í†µê³„ ===")
print(f"í‰ê·  ì¼ë³„ ë³€ë™ì„±(bp): {df['Volatility'].mean():.2f}")
print(f"ìµœëŒ€ ì¼ë³„ ë³€ë™ì„±(bp): {df['Volatility'].max():.2f}")
print(f"ìµœì†Œ ì¼ë³„ ë³€ë™ì„±(bp): {df['Volatility'].min():.2f}")

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df['Volatility'].values.reshape(-1, 1))

# ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:(i + seq_length)])
        y.append(data[i + seq_length])
    return np.array(X), np.array(y)

seq_length = 20
X, y = create_sequences(scaled_data, seq_length)

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  (90:10)
train_size = int(len(X) * 0.9)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# ë‚ ì§œ ë°ì´í„° ì¤€ë¹„ (datetime ê°ì²´ë¡œ ìœ ì§€)
dates = df['Date'].values[seq_length:]
train_dates = dates[:train_size]
test_dates = dates[train_size:]

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì¶œë ¥
print("\n=== ë°ì´í„° ê¸°ê°„ ì •ë³´ ===")
print(f"ì „ì²´ ë°ì´í„° ê¸°ê°„: {pd.Timestamp(dates[0]).strftime('%Y-%m-%d')} ~ {pd.Timestamp(dates[-1]).strftime('%Y-%m-%d')}")
print(f"í•™ìŠµ ë°ì´í„° ê¸°ê°„: {pd.Timestamp(dates[0]).strftime('%Y-%m-%d')} ~ {pd.Timestamp(dates[train_size-1]).strftime('%Y-%m-%d')}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ê°„: {pd.Timestamp(dates[train_size]).strftime('%Y-%m-%d')} ~ {pd.Timestamp(dates[-1]).strftime('%Y-%m-%d')}")

# LSTM ëª¨ë¸ êµ¬ì¶•
model = Sequential([
    LSTM(50, activation='relu', input_shape=(seq_length, 1), return_sequences=True),
    Dropout(0.2),
    LSTM(50, activation='relu'),
    Dropout(0.2),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse')

# ëª¨ë¸ í•™ìŠµ
history = model.fit(X_train, y_train, 
                   epochs=50, 
                   batch_size=32, 
                   validation_split=0.1,
                   verbose=1)

# ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰
all_predictions = []
for i in range(len(X)):
    pred = model.predict(X[i:i+1], verbose=0)
    all_predictions.append(pred[0][0])
all_predictions = np.array(all_predictions)

# ì˜ˆì¸¡ê°’ ì—­ë³€í™˜
all_predictions = scaler.inverse_transform(all_predictions.reshape(-1, 1)).flatten()
y_actual = scaler.inverse_transform(y).flatten()

# RMSE ê³„ì‚°
train_rmse = np.sqrt(np.mean((y_actual[:train_size] - all_predictions[:train_size]) ** 2))
test_rmse = np.sqrt(np.mean((y_actual[train_size:] - all_predictions[train_size:]) ** 2))
total_rmse = np.sqrt(np.mean((y_actual - all_predictions) ** 2))

# MAE (Mean Absolute Error) ê³„ì‚°
train_mae = mean_absolute_error(y_actual[:train_size], all_predictions[:train_size])
test_mae = mean_absolute_error(y_actual[train_size:], all_predictions[train_size:])

# RÂ² score ê³„ì‚°
train_r2 = r2_score(y_actual[:train_size], all_predictions[:train_size])
test_r2 = r2_score(y_actual[train_size:], all_predictions[train_size:])

# ê²°ê³¼ í…Œì´ë¸” í™•ì¥
results_table = [
    ['êµ¬ë¶„', 'RMSE', 'MAE', 'RÂ²'],
    ['í•™ìŠµ ë°ì´í„°', f'{train_rmse:.4f}', f'{train_mae:.4f}', f'{train_r2:.4f}'],
    ['í…ŒìŠ¤íŠ¸ ë°ì´í„°', f'{test_rmse:.4f}', f'{test_mae:.4f}', f'{test_r2:.4f}'],
    ['ì „ì²´ ë°ì´í„°', f'{total_rmse:.4f}', f'{mean_absolute_error(y_actual, all_predictions):.4f}', 
     f'{r2_score(y_actual, all_predictions):.4f}']
]
print("\n=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")
print(tabulate(results_table, headers='firstrow', tablefmt='grid'))

# ë©”ì¸ ê·¸ë˜í”„: ì‹¤ì œ vs ì˜ˆì¸¡ ë³€ë™ì„± + ê³ ë³€ë™ì„± êµ¬ê°„
plt.figure(figsize=(15, 6))
plt.plot(dates, y_actual, label='ì‹¤ì œ ì¼ë³„ ë³€ë™ì„±', alpha=0.7)
plt.plot(dates, all_predictions, label='ì˜ˆì¸¡ ì¼ë³„ ë³€ë™ì„±', alpha=0.7)
plt.axvline(x=dates[train_size], color='r', linestyle='--', 
            label=f'í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° êµ¬ë¶„\n({pd.Timestamp(dates[train_size]).strftime("%Y-%m-%d")})')

# ë³€ë™ì„± ì„ê³„ê°’ ì„¤ì • ë° ê³ ë³€ë™ì„± êµ¬ê°„ í‘œì‹œ
threshold = np.percentile(y_actual, 75)  # ìƒìœ„ 25% ê¸°ì¤€
plt.axhline(y=threshold, color='g', linestyle='--', 
            label=f'ê³ ë³€ë™ì„± ì„ê³„ê°’ ({threshold:.2f}bp, ìƒìœ„ 25% ìˆ˜ì¤€)')

# ê³ ë³€ë™ì„± ì˜ˆì¸¡ êµ¬ê°„ í•˜ì´ë¼ì´íŠ¸
high_vol_mask = all_predictions >= threshold
plt.fill_between(dates, 0, all_predictions, where=high_vol_mask, 
                alpha=0.3, color='red', label='ê³ ë³€ë™ì„± ì˜ˆì¸¡ êµ¬ê°„')

# ë§ˆì§€ë§‰ ë°ì´í„° í¬ì¸íŠ¸ì— ìˆ˜ì¹˜ í‘œì‹œ
last_date = dates[-1]
last_actual = y_actual[-1]
last_pred = all_predictions[-1]
plt.annotate(f'ì‹¤ì œ: {last_actual:.2f}bp',
            xy=(last_date, last_actual),
            xytext=(10, 10),
            textcoords='offset points',
            ha='left',
            va='bottom')
plt.annotate(f'ì˜ˆì¸¡: {last_pred:.2f}bp',
            xy=(last_date, last_pred),
            xytext=(10, -10),
            textcoords='offset points',
            ha='left',
            va='top')

plt.title('êµ­ê³ ì±„ ê¸ˆë¦¬ ì¼ë³„ ë³€ë™ì„± ì˜ˆì¸¡ ê²°ê³¼')
plt.xlabel('ë‚ ì§œ')
plt.ylabel('ì¼ë³„ ë³€ë™ì„± (bp)')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.tight_layout()
plt.show()

# ë³€ë™ì„± êµ­ë©´ ì˜ˆì¸¡ ì„±ê³¼ í†µê³„
total_high_vol_days = np.sum(y_actual >= threshold)
correct_predictions = np.sum((y_actual >= threshold) & (all_predictions >= threshold))
missed_predictions = np.sum((y_actual >= threshold) & (all_predictions < threshold))
false_alarms = np.sum((y_actual < threshold) & (all_predictions >= threshold))

print("\n=== ë³€ë™ì„± êµ­ë©´ ì˜ˆì¸¡ ì„±ê³¼ ===")
print(f"ì „ì²´ ê³ ë³€ë™ì„± ì¼ìˆ˜: {total_high_vol_days}ì¼")
print(f"ê³ ë³€ë™ì„± ì˜ˆì¸¡ ì„±ê³µ: {correct_predictions}ì¼ ({correct_predictions/total_high_vol_days*100:.1f}%)")
print(f"ê³ ë³€ë™ì„± ì˜ˆì¸¡ ì‹¤íŒ¨: {missed_predictions}ì¼ ({missed_predictions/total_high_vol_days*100:.1f}%)")
print(f"ì˜¤íƒ(False Alarm): {false_alarms}ì¼")

# í–¥í›„ ê³ ë³€ë™ì„± ì˜ˆì¸¡ êµ¬ê°„ ì¶œë ¥
future_high_vol = dates[high_vol_mask][-10:]  # ìµœê·¼ 10ê°œì˜ ê³ ë³€ë™ì„± ì˜ˆì¸¡ êµ¬ê°„
print("\n=== ìµœê·¼ ê³ ë³€ë™ì„± ì˜ˆì¸¡ êµ¬ê°„ ===")
for date in future_high_vol:
    print(f"{pd.Timestamp(date).strftime('%Y-%m-%d')}: ì˜ˆì¸¡ ë³€ë™ì„± {all_predictions[dates == date][0]:.2f}bp")

# Streamlit ì›¹ì•± ë¶€ë¶„ ì¶”ê°€
import streamlit as st
import plotly.graph_objects as go

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(
    page_title="ê¸ˆë¦¬ ë³€ë™ì„± ì˜ˆì¸¡ ëª¨ë¸",
    page_icon="ğŸ“ˆ",
    layout="wide"
)

# ì œëª© ë° ì„¤ëª…
st.title("êµ­ê³ ì±„ ê¸ˆë¦¬ ë³€ë™ì„± ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ")
st.markdown("LSTM ëª¨ë¸ì„ í™œìš©í•œ ê¸ˆë¦¬ ë³€ë™ì„± ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.")

# ë©”ì¸ ëŒ€ì‹œë³´ë“œ êµ¬ì„±
col1, col2 = st.columns(2)

with col1:
    st.subheader("ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ")
    # ê²°ê³¼ í…Œì´ë¸”ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
    results_df = pd.DataFrame(results_table[1:], columns=results_table[0])
    st.dataframe(results_df)

with col2:
    st.subheader("ë³€ë™ì„± êµ­ë©´ ì˜ˆì¸¡ ì„±ê³¼")
    metrics_col1, metrics_col2 = st.columns(2)
    with metrics_col1:
        st.metric("ì „ì²´ ê³ ë³€ë™ì„± ì¼ìˆ˜", f"{total_high_vol_days}ì¼")
        st.metric("ì˜ˆì¸¡ ì„±ê³µë¥ ", f"{correct_predictions/total_high_vol_days*100:.1f}%")
    with metrics_col2:
        st.metric("ì˜ˆì¸¡ ì‹¤íŒ¨", f"{missed_predictions}ì¼")
        st.metric("ì˜¤íƒ(False Alarm)", f"{false_alarms}ì¼")

# Plotlyë¥¼ ì‚¬ìš©í•œ ì¸í„°ë™í‹°ë¸Œ ê·¸ë˜í”„
st.subheader("ë³€ë™ì„± ì˜ˆì¸¡ ê²°ê³¼")
fig = go.Figure()

# ì‹¤ì œ ë³€ë™ì„±ê³¼ ì˜ˆì¸¡ ë³€ë™ì„± ë¼ì¸
fig.add_trace(go.Scatter(
    x=dates, 
    y=y_actual,
    name="ì‹¤ì œ ì¼ë³„ ë³€ë™ì„±",
    line=dict(color='blue', width=1)
))

fig.add_trace(go.Scatter(
    x=dates, 
    y=all_predictions,
    name="ì˜ˆì¸¡ ì¼ë³„ ë³€ë™ì„±",
    line=dict(color='red', width=1)
))

# ê³ ë³€ë™ì„± ì„ê³„ê°’ ë¼ì¸ ì¶”ê°€
fig.add_trace(go.Scatter(
    x=dates,
    y=[threshold] * len(dates),
    name=f'ê³ ë³€ë™ì„± ì„ê³„ê°’ ({threshold:.2f}bp, ìƒìœ„ 25% ìˆ˜ì¤€)',
    line=dict(color='green', width=1, dash='dash')
))

# ê³ ë³€ë™ì„± êµ¬ê°„ í‘œì‹œ (ì‹¤ì œ ë³€ë™ì„±ì´ ì„ê³„ê°’ì„ ë„˜ëŠ” êµ¬ê°„)
high_vol_mask = y_actual >= threshold
high_vol_dates = []
for i in range(len(dates)-1):
    if high_vol_mask[i]:
        fig.add_vrect(
            x0=dates[i],
            x1=dates[i+1],
            fillcolor="rgba(255, 0, 0, 0.2)",
            layer="below",
            line_width=0,
            name="ê³ ë³€ë™ì„± êµ¬ê°„",
            showlegend=False
        )

fig.update_layout(
    title="êµ­ê³ ì±„ ê¸ˆë¦¬ ì¼ë³„ ë³€ë™ì„± ì˜ˆì¸¡ ê²°ê³¼",
    xaxis_title="ë‚ ì§œ",
    yaxis_title="ì¼ë³„ ë³€ë™ì„± (bp)",
    hovermode='x unified',
    showlegend=True
)

st.plotly_chart(fig, use_container_width=True)

# ìµœê·¼ 5ì¼ì˜ ì‹¤ì œ/ì˜ˆì¸¡ ë³€ë™ì„± ì •ë³´ë¥¼ í‘œë¡œ í‘œì‹œ (ì—­ìˆœ)
st.subheader("ìµœê·¼ 5ì¼ ë³€ë™ì„± ì •ë³´")
recent_dates = dates[-5:][::-1]  # ì—­ìˆœìœ¼ë¡œ ì •ë ¬
recent_actual = y_actual[-5:][::-1]
recent_pred = all_predictions[-5:][::-1]

# ë°ì´í„° ì¤€ë¹„
data = []
for date, act, pred in zip(recent_dates, recent_actual, recent_pred):
    date_str = pd.Timestamp(date).strftime('%Y-%m-%d')
    data.extend([
        [date_str, 'ì‹¤ì œ', f'{act:.2f}'],
        [date_str, 'ì˜ˆì¸¡', f'{pred:.2f}']
    ])

recent_vol_df = pd.DataFrame(data, columns=['ë‚ ì§œ', 'êµ¬ë¶„', 'ë³€ë™ì„±(bp)'])
st.dataframe(recent_vol_df)

# ìµœê·¼ ê³ ë³€ë™ì„± ì˜ˆì¸¡ êµ¬ê°„
st.subheader("ìµœê·¼ ê³ ë³€ë™ì„± ì˜ˆì¸¡ êµ¬ê°„")
recent_predictions = pd.DataFrame({
    'ë‚ ì§œ': [pd.Timestamp(date).strftime('%Y-%m-%d') for date in future_high_vol],
    'ì˜ˆì¸¡ ë³€ë™ì„±(bp)': [all_predictions[dates == date][0] for date in future_high_vol]
})
st.dataframe(recent_predictions)

# ì‹¤í–‰ ë°©ë²•ì„ ì¼ë°˜ ë¬¸ìì—´ë¡œ í‘œì‹œ
print("""
ì‹¤í–‰ ë°©ë²•:
1. í„°ë¯¸ë„/ëª…ë ¹ í”„ë¡¬í”„íŠ¸ë¥¼ ì—´ê³ 
2. íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë¡œ ì´ë™:
   cd C:/Users/infomax/Desktop/Cursor/model
3. ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰:
   streamlit run volatility_predictor_v1.1.py
4. ì»¤ë°‹ í›„ í‘¸ì‰¬
""")